{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка Mistral-7B-v0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b72a87a586fb491c86dcaa4a434e655b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "device = torch.device('cuda')\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=device, quantization_config=quantization_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Отключение логирования ошибок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'models.prompts' from '/home/user/Hacks/DigitalBreakthrough-AIAssistant/models/prompts.py'>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models import prompts\n",
    "import importlib\n",
    "importlib.reload(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'models.catboosty' from '/home/user/Hacks/DigitalBreakthrough-AIAssistant/models/catboosty.py'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models import catboosty\n",
    "import importlib\n",
    "importlib.reload(catboosty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catboosty.filter_question(['налоговый вычет можно получить?'], catboosty.model_filter_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Инференс с Streaming'ом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [df.iloc[i]['Question'] for i in range(len(df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = prompts.classify_prompt(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = [int(x.split('- ')[-1]) for x in st.split('\\n') if len(x) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_subm = pd.read_csv('./sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_subm['answer_class'] = h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_subm.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_obscene_words(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        obscene_words = [word.strip().lower() for word in file.readlines()]\n",
    "    return obscene_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_obscene(text, obscene_words):\n",
    "    text_lower = text.lower()\n",
    "    for word in obscene_words:\n",
    "        if word in text_lower:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "obscene_words = load_obscene_words('data/words.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextIteratorStreamer\n",
    "from threading import Thread\n",
    "\n",
    "# streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, timeout=120)\n",
    "\n",
    "def process(message: str, context: str):\n",
    "    # if check_for_obscene(message, obscene_words):\n",
    "        # return False\n",
    "    # variants = catboosty.pipeline_predict2([message], catboosty.model_cl_category, catboosty.model_cl_answer, catboosty.tfidf_vectorizer)\n",
    "    # encodeds = tokenizer.encode(prompts.top_3_prompt(message, variants), return_tensors='pt')\n",
    "    # model_inputs = encodeds.to(device)\n",
    "    # gen_ids = model.generate(model_inputs, max_new_tokens=100, do_sample=True)\n",
    "    # decoded = tokenizer.batch_decode(gen_ids)\n",
    "\n",
    "    # ans = decoded[0].split('[/INST]')[-1]\n",
    "\n",
    "    encodeds = tokenizer.encode(prompts.classify_prompt(l), return_tensors=\"pt\")\n",
    "\n",
    "    model_inputs = encodeds.to(device)\n",
    "\n",
    "    model_inputs = encodeds.to(device)\n",
    "    gen_ids = model.generate(model_inputs, max_new_tokens=10000, do_sample=True)\n",
    "    decoded = tokenizer.batch_decode(gen_ids)\n",
    "\n",
    "    print(decoded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7863\n",
      "Running on public URL: https://cb56e54db856074214.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://cb56e54db856074214.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Exception in thread Thread-26 (generate):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/user/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 761, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/user/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/user/.local/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1544, in generate\n",
      "    return self.greedy_search(\n",
      "  File \"/home/user/.local/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2404, in greedy_search\n",
      "    outputs = self(\n",
      "  File \"/home/user/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/user/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/user/.local/lib/python3.10/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "  File \"/home/user/.local/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py\", line 1157, in forward\n",
      "    outputs = self.model(\n",
      "  File \"/home/user/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/user/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/user/.local/lib/python3.10/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "  File \"/home/user/.local/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py\", line 1042, in forward\n",
      "    layer_outputs = decoder_layer(\n",
      "  File \"/home/user/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/user/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/user/.local/lib/python3.10/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "  File \"/home/user/.local/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py\", line 770, in forward\n",
      "    hidden_states = self.mlp(hidden_states)\n",
      "  File \"/home/user/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/user/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/user/.local/lib/python3.10/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "  File \"/home/user/.local/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py\", line 179, in forward\n",
      "    return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "  File \"/home/user/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/user/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/user/.local/lib/python3.10/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "  File \"/home/user/.local/lib/python3.10/site-packages/bitsandbytes/nn/modules.py\", line 431, in forward\n",
      "    out = out.to(inp_dtype)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 244.00 MiB. GPU 0 has a total capacity of 11.76 GiB of which 60.44 MiB is free. Including non-PyTorch memory, this process has 11.70 GiB memory in use. Of the allocated memory 10.56 GiB is allocated by PyTorch, and 1007.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import gradio as gr\n",
    "\n",
    "messages = \"\"\n",
    "loyalty = 100\n",
    "def slow_echo(message, history):\n",
    "    global messages, current_state\n",
    "    messages += f\"User: {message}\\n\"\n",
    "    result = process(p, [])\n",
    "    history = \"\"\n",
    "    for char in streamer:\n",
    "        history += char\n",
    "        yield history\n",
    "\n",
    "gr.ChatInterface(slow_echo, css=\"footer {visibility: hidden}\").launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-28 04:49:01,082 (__init__.py:1086 MainThread) ERROR - TeleBot: \"Infinity polling exception: A request to the Telegram API was unsuccessful. Error code: 404. Description: Not Found\"\n",
      "2024-04-28 04:49:01,083 (__init__.py:1088 MainThread) ERROR - TeleBot: \"Exception traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/.local/lib/python3.10/site-packages/telebot/__init__.py\", line 1081, in infinity_polling\n",
      "    self.polling(non_stop=True, timeout=timeout, long_polling_timeout=long_polling_timeout,\n",
      "  File \"/home/user/.local/lib/python3.10/site-packages/telebot/__init__.py\", line 1166, in polling\n",
      "    logger.info('Starting your bot with username: [@%s]', self.user.username)\n",
      "  File \"/home/user/.local/lib/python3.10/site-packages/telebot/__init__.py\", line 293, in user\n",
      "    self._user = self.get_me()\n",
      "  File \"/home/user/.local/lib/python3.10/site-packages/telebot/__init__.py\", line 1353, in get_me\n",
      "    apihelper.get_me(self.token)\n",
      "  File \"/home/user/.local/lib/python3.10/site-packages/telebot/apihelper.py\", line 201, in get_me\n",
      "    return _make_request(token, method_url)\n",
      "  File \"/home/user/.local/lib/python3.10/site-packages/telebot/apihelper.py\", line 167, in _make_request\n",
      "    json_result = _check_result(method_name, result)\n",
      "  File \"/home/user/.local/lib/python3.10/site-packages/telebot/apihelper.py\", line 194, in _check_result\n",
      "    raise ApiTelegramException(method_name, result, result_json)\n",
      "telebot.apihelper.ApiTelegramException: A request to the Telegram API was unsuccessful. Error code: 404. Description: Not Found\n",
      "\"\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mApiTelegramException\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/telebot/__init__.py:1081\u001b[0m, in \u001b[0;36mTeleBot.infinity_polling\u001b[0;34m(self, timeout, skip_pending, long_polling_timeout, logger_level, allowed_updates, restart_on_change, path_to_watch, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1081\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnon_stop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlong_polling_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlong_polling_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mlogger_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger_level\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallowed_updates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallowed_updates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrestart_on_change\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[43m                 \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/telebot/__init__.py:1166\u001b[0m, in \u001b[0;36mTeleBot.polling\u001b[0;34m(self, non_stop, skip_pending, interval, timeout, long_polling_timeout, logger_level, allowed_updates, none_stop, restart_on_change, path_to_watch)\u001b[0m\n\u001b[1;32m   1164\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setup_change_detector(path_to_watch)\n\u001b[0;32m-> 1166\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStarting your bot with username: [@\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser\u001b[49m\u001b[38;5;241m.\u001b[39musername)\n\u001b[1;32m   1168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthreaded:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/telebot/__init__.py:293\u001b[0m, in \u001b[0;36mTeleBot.user\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_user:\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_user \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_me\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_user\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/telebot/__init__.py:1353\u001b[0m, in \u001b[0;36mTeleBot.get_me\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1347\u001b[0m \u001b[38;5;124;03mA simple method for testing your bot's authentication token. Requires no parameters.\u001b[39;00m\n\u001b[1;32m   1348\u001b[0m \u001b[38;5;124;03mReturns basic information about the bot in form of a User object.\u001b[39;00m\n\u001b[1;32m   1349\u001b[0m \n\u001b[1;32m   1350\u001b[0m \u001b[38;5;124;03mTelegram documentation: https://core.telegram.org/bots/api#getme\u001b[39;00m\n\u001b[1;32m   1351\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1352\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m types\u001b[38;5;241m.\u001b[39mUser\u001b[38;5;241m.\u001b[39mde_json(\n\u001b[0;32m-> 1353\u001b[0m     \u001b[43mapihelper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_me\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1354\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/telebot/apihelper.py:201\u001b[0m, in \u001b[0;36mget_me\u001b[0;34m(token)\u001b[0m\n\u001b[1;32m    200\u001b[0m method_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgetMe\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 201\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_url\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/telebot/apihelper.py:167\u001b[0m, in \u001b[0;36m_make_request\u001b[0;34m(token, method_name, method, params, files)\u001b[0m\n\u001b[1;32m    165\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe server returned: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(result\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[0;32m--> 167\u001b[0m json_result \u001b[38;5;241m=\u001b[39m \u001b[43m_check_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m json_result:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/telebot/apihelper.py:194\u001b[0m, in \u001b[0;36m_check_result\u001b[0;34m(method_name, result)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result_json[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mok\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m--> 194\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ApiTelegramException(method_name, result, result_json)\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result_json\n",
      "\u001b[0;31mApiTelegramException\u001b[0m: A request to the Telegram API was unsuccessful. Error code: 404. Description: Not Found",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m     result \u001b[38;5;241m=\u001b[39m process(prompts\u001b[38;5;241m.\u001b[39mget_prompt(message\u001b[38;5;241m.\u001b[39mtext), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[/INST]\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     14\u001b[0m     bot\u001b[38;5;241m.\u001b[39mreply_to(message, result)\n\u001b[0;32m---> 16\u001b[0m \u001b[43mbot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfinity_polling\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/telebot/__init__.py:1089\u001b[0m, in \u001b[0;36mTeleBot.infinity_polling\u001b[0;34m(self, timeout, skip_pending, long_polling_timeout, logger_level, allowed_updates, restart_on_change, path_to_watch, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1087\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m logger_level \u001b[38;5;129;01mand\u001b[39;00m logger_level \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mDEBUG:\n\u001b[1;32m   1088\u001b[0m         logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mException traceback:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, traceback\u001b[38;5;241m.\u001b[39mformat_exc())\n\u001b[0;32m-> 1089\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1090\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logger_level \u001b[38;5;129;01mand\u001b[39;00m logger_level \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mINFO:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import telebot\n",
    "\n",
    "\n",
    "bot = telebot.TeleBot('SECRET')\n",
    "\n",
    "@bot.message_handler(commands=['start', 'help'])\n",
    "def greet(message):\n",
    "    bot.send_message(message.chat.id, 'Привет! Я бот техподдержки GeekBrains, чем могу помочь?')\n",
    "\n",
    "@bot.message_handler(func=lambda message: True)\n",
    "def echo_all(message):\n",
    "    bot.send_chat_action(message.chat.id, 'typing')\n",
    "    result = process(prompts.get_prompt(message.text), 'user').split('[/INST]')[-1].strip()\n",
    "    bot.reply_to(message, result)\n",
    "\n",
    "bot.infinity_polling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "02ead6bf38c64279aa4b46f586f7761c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "27ff8c5728544784970324fbd3bd5f52": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_8043f654e0ee48d9b809d3d22009612d",
       "max": 3,
       "style": "IPY_MODEL_02ead6bf38c64279aa4b46f586f7761c",
       "value": 3
      }
     },
     "28c8911124c64df2a01bc53ecd1593b3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "2c534a7d26fa453e8175ab51bac15ee4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "38839135c14e4d1fb6d3429c4d927f17": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "687db814dde74da7be2bd25a60f82ab4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_94154c5937b44fabafe636a27fd6e6c0",
       "style": "IPY_MODEL_28c8911124c64df2a01bc53ecd1593b3",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "8043f654e0ee48d9b809d3d22009612d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "94154c5937b44fabafe636a27fd6e6c0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b72a87a586fb491c86dcaa4a434e655b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_687db814dde74da7be2bd25a60f82ab4",
        "IPY_MODEL_27ff8c5728544784970324fbd3bd5f52",
        "IPY_MODEL_c8528fe43f5d4819aa96c575c887e087"
       ],
       "layout": "IPY_MODEL_38839135c14e4d1fb6d3429c4d927f17"
      }
     },
     "c2f4d87175e44be68fd257d702f8cb9b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c8528fe43f5d4819aa96c575c887e087": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_c2f4d87175e44be68fd257d702f8cb9b",
       "style": "IPY_MODEL_2c534a7d26fa453e8175ab51bac15ee4",
       "value": " 3/3 [00:34&lt;00:00, 10.97s/it]"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
